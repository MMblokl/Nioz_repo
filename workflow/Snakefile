"""
Metagenomics workflow for estimating abundance from metagenomic shotgun reads.
All provided samples are assumed to be replicates of each other unless specified.
Please read the README before using to make sure all tools can be found properly.
Company: NIOZ
Part of an internship project for the Hogeschool Leiden
Author: Mats Blokland
"""

project = config["PROJECT"]

#Hier moet de output van de laatste rule in de input staan met een expand.
rule all:
  input:
    expand("{project}/gtdbtk/{SAMPLE}/classification.tsv", project=project, SAMPLE=config["READS"]["ids"])
    

#
##################
# File structure #
##################
# This rule initates the sample structure usinig the script "init_sample.sh".

rule structure:
  input:
    fw = config["READS"]["dir"] +"{SAMPLE}_1.fastq",
    rv = config["READS"]["dir"] +"{SAMPLE}_2.fastq"
  output:
    f1="{project}/samples/{SAMPLE}/rawdata/fw.fastq",
    f2="{project}/samples/{SAMPLE}/rawdata/rv.fastq"
  shell:
    "scripts/init_sample.sh {wildcards.project} {wildcards.SAMPLE} {input.fw} {input.rv}"


assembly_threads = config["assembly"]["threads"]
#
############
# Assembly #
############
# Assemble all read pairs using metaspades.

rule assemble:
  input:
    fw = "{project}/samples/{SAMPLE}/rawdata/fw.fastq",
    rv = "{project}/samples/{SAMPLE}/rawdata/rv.fastq"
  output:
    "{project}/metaspades/{SAMPLE}/contigs.fasta"
  shell:
    "metaspades.py -1 {input.fw} -2 {input.rv} "
    "-o {wildcards.project}/metaspades/{wildcards.SAMPLE}/ "
    "-t {assembly_threads}" 


min_contiglen = config["binning"]["min_contiglen"]
mapping_threads = config["mapping"]["threads"]
###########
# Mapping #
###########
#There are two options for the rules.
#
#1. The option "replicates" is set to TRUE, which means that every read pair is
#   aligned to every contig file. This will create a lot of alignments, which
#   will take a very long time. This increases the amount of reference for the
#   binning phase, so it is recommended to use this option if possible.
#
#2. The reads are mapped back to their own respective contigs.
#   This is when the option "replicates" is set to False or anything else than
#   True.

#Remove all contigs smaller than the provided min_contiglen
rule filter_contigs:
  input:
    contigs = "{project}/metaspades/{SAMPLE}/contigs.fasta"
  output:
    filt_contigs = "{project}/bbmap/{SAMPLE}/contigs_filtered.fasta"
  shell:
    "seqtk seq -L {min_contiglen} {input.contigs} > {output.filt_contigs}"


#Create the reference so that snakemake does not attempt to align to an index that is not yet completed.
rule mapping_reference:
  input:
    contigs = "{project}/bbmap/{SAMPLE}/contigs_filtered.fasta"
  output:
    reference = "{project}/bbmap/{SAMPLE}/ref/genome/1/summary.txt"
  shell:
    "bbmap.sh ref={input.contigs} "
    "path={wildcards.project}/bbmap/{wildcards.SAMPLE}/ "
    "-threads={mapping_threads}"
    

#Map the reads back to the contigs
if config["mapping"]["replicates"]:

  rule mapping:
    input:
      expand("{project}/samples/{sample2}/rawdata/fw.fastq", project=project, sample2=config["READS"]["ids"]),
      fw = "{project}/samples/{sample2}/rawdata/fw.fastq",
      rv = "{project}/samples/{sample2}/rawdata/rv.fastq",
      contigs = "{project}/bbmap/{SAMPLE}/contigs_filtered.fasta",
      reference = "{project}/bbmap/{SAMPLE}/ref/genome/1/summary.txt"
    output:
      samfile = "{project}/bbmap/{SAMPLE}/{sample2}.sam"
    shell:
      "bbmap.sh ref={input.contigs} "
      "path={wildcards.project}/bbmap/{wildcards.SAMPLE} " #This line is to make sure that the index isn't created more than once
      "in={input.fw} in2={input.rv} "
      "-out={output.samfile} "
      "-threads={mapping_threads}"

  rule bam_convert:
    input:
      samfile = "{project}/bbmap/{SAMPLE}/{sample2}.sam"
    output:
      bamfile = "{project}/bbmap/{SAMPLE}/{sample2}.BAM"
    shell:
      "samtools view -b {input.samfile} "
      "-o {output.bamfile}"
  
  rule bam_sort:
    input:
      bamfile = "{project}/bbmap/{SAMPLE}/{sample2}.BAM"
    output:
      sortedfile = "{project}/bbmap/{SAMPLE}/{sample2}_sorted.bam"
    shell:
      "samtools sort {input.bamfile} "
      "-o {output.sortedfile}"
  
  rule bam_index:
    input:
      sortedfile = "{project}/bbmap/{SAMPLE}/{sample2}_sorted.bam"
    output:
      indexfile = "{project}/bbmap/{SAMPLE}/{sample2}_sorted.bam.bai"
    shell:
      "samtools index -b -@ {mapping_threads} "
      "{input.sortedfile} "
      "{output.indexfile}"
  
  #This rule here makes sure that all the replicates are mapped against each other.
  rule request_mapping:
    input:
      expand("{project}/bbmap/{SAMPLE}/{sample2}_sorted.bam.bai", project=project, SAMPLE=config["READS"]["ids"], sample2=config["READS"]["ids"])
    output:
      "{project}/bbmap/{SAMPLE}/mapping_flag.txt"
    shell:
      "touch {wildcards.project}/bbmap/{wildcards.SAMPLE}/mapping_flag.txt"

else:
  
  rule mapping:
    input:
      fw = "{project}/samples/{SAMPLE}/rawdata/fw.fastq",
      rv = "{project}/samples/{SAMPLE}/rawdata/rv.fastq",
      contigs = "{project}/bbmap/{SAMPLE}/contigs_filtered.fasta",
      reference = "{project}/bbmap/{SAMPLE}/ref/genome/1/summary.txt"
    output:
      "{project}/bbmap/{SAMPLE}/{SAMPLE}.sam"
    shell:
      "bbmap.sh ref={input.contigs} "
      "path={wildcards.project}/bbmap/{wildcards.SAMPLE}"
      "in={input.fw} in2={input.rv} "
      "-out={wildcards.project}/bbmap/{wildcards.SAMPLE}/{wildcards.SAMPLE}.sam "
      "-threads={mapping_threads}"
  
  rule bam_convert:
    input:
      "{project}/bbmap/{SAMPLE}/{SAMPLE}.sam"
    output:
      "{project}/bbmap/{SAMPLE}/{SAMPLE}.bam"
    shell:
      "samtools view -b {wildcards.project}/bbmap/{wildcards.SAMPLE}/{wildcards.SAMPLE}.sam "
      "-o {wildcards.project}/bbmap/{wildcards.SAMPLE}/{wildcards.SAMPLE}.bam"
  
  rule bam_sort:
    input:
      "{project}/bbmap/{SAMPLE}/{SAMPLE}.bam"
    output:
      "{project}/bbmap/{SAMPLE}/{SAMPLE}_sorted.bam"
    shell:
      "samtools sort {wildcards.project}/bbmap/{wildcards.SAMPLE}/{wildcards.SAMPLE}.bam "
      "-o {wildcards.project}/bbmap/{wildcards.SAMPLE}/{wildcards.SAMPLE}_sorted.bam"
  
  rule bam_index:
    input:
      "{project}/bbmap/{SAMPLE}/{SAMPLE}_sorted.bam"
    output:
      "{project}/bbmap/{SAMPLE}/{SAMPLE}_sorted.bam.bai"
    shell:
      "samtools index -b -@ " + config["mapping"]["threads"] +
      " {wildcards.project}/bbmap/{wildcards.SAMPLE}/{wildcards.SAMPLE}_sorted.bam "
      "{wildcards.project}/bbmap/{wildcards.SAMPLE}/{wildcards.SAMPLE}_sorted.bam.bai"
  
  #Same rule for the replicates to make sure both options can equally be used.
  rule request_mapping:
    input:
      rules.bam_index.output
    output:
      "{project}/bbmap/{SAMPLE}/mapping_flag.txt"
    shell:
      "touch {wildcards.project}/bbmap/{wildcards.SAMPLE}/mapping_flag.txt"

binning_kmerlen = config["binning"]["kmer_len"]
binning_threads = config["binning"]["threads"]
metabinner_env = config["binning"]["metabinner_env"]
comp_cutoff = config["binning"]["cutoff_comp"]
cont_cutoff = config["binning"]["cutoff_cont"]
#
###########
# Binning #
###########
# Hello

#Generates the contig depth file from all alignments in the bbmap dir.
rule gen_depth:
  input:
    "{project}/bbmap/{SAMPLE}/mapping_flag.txt"
  output:
    "{project}/metabinner/{SAMPLE}/mb2_master_depth.txt"
  shell:
    "./scripts/metabinner_scripts/jgi_summarize_bam_contig_depths "
    "--outputDepth {wildcards.project}/metabinner/{wildcards.SAMPLE}/mb2_master_depth.txt "
    "--noIntraDepthVariance {wildcards.project}/bbmap/{wildcards.SAMPLE}/*.bam"

#Generates a coverage file from the contig depth file.
rule gen_coverage:
  input:
    "{project}/metabinner/{SAMPLE}/mb2_master_depth.txt"
  output:
    "{project}/metabinner/{SAMPLE}/coverage_profile.tsv"
  shell:
    """cat {wildcards.project}/metabinner/{wildcards.SAMPLE}/mb2_master_depth.txt \
    | awk '{{if ($2>'"1000"') print $0 }}' | \
    cut -f -1,4- > {wildcards.project}/metabinner/{wildcards.SAMPLE}/coverage_profile.tsv"""


#Create kmer profile and run metabinner
rule metabinner:
  input:
    "{project}/bbmap/{SAMPLE}/contigs_filtered.fasta",
    "{project}/bbmap/{SAMPLE}/{SAMPLE}_sorted.bam.bai",
    "{project}/metabinner/{SAMPLE}/coverage_profile.tsv"
  output:
    "{project}/metabinner/{SAMPLE}/metabinner_res/result.log"
  shell:
    "bash ./scripts/mbinner.wrap "
    "{wildcards.project}/bbmap/{wildcards.SAMPLE}/contigs_filtered.fasta {min_contiglen} "
    "{binning_kmerlen} {wildcards.project}/metabinner/{wildcards.SAMPLE} "
    "{metabinner_env} {binning_threads}"


#This rule runs the checkM evaluation. For some reason, they decided to put the
#final bins in a very long directory chain, so while it looks weird, it's the correct
#bin directory.
rule checkm_eval:
  input:
    "{project}/metabinner/{SAMPLE}/metabinner_res/result.log"
  output:
    "{project}/checkm/{SAMPLE}/lineage.ms"
  shell:
    "checkm lineage_wf -t {binning_threads} -x fna "
    "{wildcards.project}/metabinner/{wildcards.SAMPLE}/metabinner_res/ensemble_res/greedy_cont_weight_3_mincomp_50.0_maxcont_15.0_bins/ensemble_3logtrans/addrefined2and3comps/greedy_cont_weight_3_mincomp_50.0_maxcont_15.0_bins/ "
    "{wildcards.project}/checkm/{wildcards.SAMPLE}/"


#Filter the bins based on the checkM evaluation. The cutoff can be changed in the config.
rule filter_bins:
  input:
    "{project}/checkm/{SAMPLE}/lineage.ms"
  output:
    "{project}/filtered_bins/{SAMPLE}/bin_stats.tsv"
  shell:
    "python3 scripts/filter_bins.py "
    "{wildcards.project}/checkm/{wildcards.SAMPLE}/storage/bin_stats_ext.tsv "
    "{comp_cutoff} {cont_cutoff} "
    "{wildcards.project}/metabinner/{wildcards.SAMPLE}/metabinner_res/ensemble_res/greedy_cont_weight_3_mincomp_50.0_maxcont_15.0_bins/ensemble_3logtrans/addrefined2and3comps/greedy_cont_weight_3_mincomp_50.0_maxcont_15.0_bins/ "
    "{wildcards.project}/filtered_bins/{wildcards.SAMPLE}/"

class_cpus = config["classification"]["cpus"]
#
################
#Classification
################
#

#Runs the classification with gtdbtk
rule gtdbtk:
  input:
    "{project}/filtered_bins/{SAMPLE}/bin_stats.tsv"
  output:
    "{project}/gtdbtk/{SAMPLE}/gtdbtk.log"
  shell:
    "bash ./scripts/gtdbtk.wrap {wildcards.project}/filtered_bins/{wildcards.SAMPLE}/ "
    "{wildcards.project}/gtdbtk/{wildcards.SAMPLE}/ {class_cpus}"


#Runs a script to collect the classifications into one file, which is more readable.
rule get_classification:
  input:
    "{project}/gtdbtk/{SAMPLE}/gtdbtk.log"
  output:
    "{project}/gtdbtk/{SAMPLE}/classification.tsv"
  shell:
    "python3 scripts/get_classifications.py "
    "{wildcards.project}/gtdbtk/{wildcards.SAMPLE}/classification.tsv "
    "{wildcards.project}/gtdbtk/{wildcards.SAMPLE}"


###########









