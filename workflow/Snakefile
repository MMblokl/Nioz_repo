"""
Metagenomics workflow for estimating abundance from metagenomic shotgun reads.
All provided samples are assumed to be replicates of each other unless specified.
Please read the README before using to make sure all tools are installed and can
be used properly by the workflow.
Company: NIOZ
Part of an internship project for the Hogeschool Leiden
Author: Mats Blokland
"""

project = config["PROJECT"]

rule all:
  input:
    expand("{project}/quantification/{SAMPLE}/quant_table.txt", project=project, SAMPLE=config["READS"]["ids"])
    

#
##################
# File structure #
##################
# This rule initates the sample structure using the script "init_sample.sh".

rule structure:
  input:
    fw = config["READS"]["dir"] +"{SAMPLE}_1.fastq",
    rv = config["READS"]["dir"] +"{SAMPLE}_2.fastq"
  output:
    f1="{project}/samples/{SAMPLE}/rawdata/fw.fastq",
    f2="{project}/samples/{SAMPLE}/rawdata/rv.fastq"
  shell:
    "bash ./scripts/init_sample.sh {wildcards.project} {wildcards.SAMPLE} {input.fw} {input.rv}"


assembly_threads = config["assembly"]["threads"]
#
############
# Assembly #
############
# Assemble all read pairs using metaspades.

rule assemble:
  input:
    fw = "{project}/samples/{SAMPLE}/rawdata/fw.fastq",
    rv = "{project}/samples/{SAMPLE}/rawdata/rv.fastq"
  output:
    "{project}/metaspades/{SAMPLE}/contigs.fasta"
  shell:
    "metaspades.py -1 {input.fw} -2 {input.rv} "
    "-o {wildcards.project}/metaspades/{wildcards.SAMPLE}/ "
    "-t {assembly_threads}" 


min_contiglen = config["binning"]["min_contiglen"]
mapping_threads = config["mapping"]["threads"]
###########
# Mapping #
###########
#There are two options for the rules.
#
#1. The option "replicates" is set to TRUE, which means that every read pair is
#   aligned to every contig file. This will create a lot of alignments, which
#   will take a very long time. This increases the amount of reference for the
#   binning phase, so it is recommended to use this option if possible.
#
#2. The reads are mapped back to their own respective contigs.
#   This is when the option "replicates" is set to False or anything else than
#   True.

#Remove all contigs smaller than the provided min_contiglen
rule filter_contigs:
  input:
    contigs = "{project}/metaspades/{SAMPLE}/contigs.fasta"
  output:
    filt_contigs = "{project}/bbmap/{SAMPLE}/contigs_filtered.fasta"
  shell:
    "reformat.sh {input.contigs} fastaminlen={min_contiglen} out={output.filt_contigs}"


#Create the reference so that snakemake does not attempt to align to an index that is not yet completed.
rule mapping_reference:
  input:
    contigs = "{project}/bbmap/{SAMPLE}/contigs_filtered.fasta"
  output:
    reference = "{project}/bbmap/{SAMPLE}/ref/genome/1/summary.txt"
  shell:
    "bbmap.sh ref={input.contigs} "
    "path={wildcards.project}/bbmap/{wildcards.SAMPLE}/ "
    "-threads={mapping_threads}"
    

#Map the reads back to the contigs
if config["mapping"]["replicates"]:

  rule mapping:
    input:
      expand("{project}/samples/{sample2}/rawdata/fw.fastq", project=project, sample2=config["READS"]["ids"]),
      fw = "{project}/samples/{sample2}/rawdata/fw.fastq",
      rv = "{project}/samples/{sample2}/rawdata/rv.fastq",
      contigs = "{project}/bbmap/{SAMPLE}/contigs_filtered.fasta",
      reference = "{project}/bbmap/{SAMPLE}/ref/genome/1/summary.txt" #For information, this is not actually used in running BBmap, it is just used as a flag, as this file signifies the reference is complete.
    output:
      samfile = temp("{project}/bbmap/{SAMPLE}/{sample2}.sam")
    shell:
      "bbmap.sh ref={input.contigs} "
      "path={wildcards.project}/bbmap/{wildcards.SAMPLE} " #This line is to make sure that the index isn't created more than once
      "in={input.fw} in2={input.rv} "
      "-out={output.samfile} "
      "-threads={mapping_threads}"

  rule bam_convert:
    input:
      samfile = "{project}/bbmap/{SAMPLE}/{sample2}.sam"
    output:
      bamfile = temp("{project}/bbmap/{SAMPLE}/{sample2}.BAM")
    shell:
      "samtools view -b {input.samfile} "
      "-o {output.bamfile}"
  
  rule bam_sort:
    input:
      bamfile = "{project}/bbmap/{SAMPLE}/{sample2}.BAM"
    output:
      sortedfile = "{project}/bbmap/{SAMPLE}/{sample2}_sorted.bam"
    shell:
      "samtools sort {input.bamfile} "
      "-o {output.sortedfile}"
  
  rule bam_index:
    input:
      sortedfile = "{project}/bbmap/{SAMPLE}/{sample2}_sorted.bam"
    output:
      indexfile = "{project}/bbmap/{SAMPLE}/{sample2}_sorted.bam.bai"
    shell:
      "samtools index -b -@ {mapping_threads} "
      "{input.sortedfile} "
      "{output.indexfile}"
  
  #This rule here makes sure that all the replicates are mapped against each other.
  rule request_mapping:
    input:
      expand("{project}/bbmap/{SAMPLE}/{sample2}_sorted.bam.bai", project=project, SAMPLE=config["READS"]["ids"], sample2=config["READS"]["ids"])
    output:
      "{project}/bbmap/{SAMPLE}/mapping_flag.txt"
    shell:
      "touch {wildcards.project}/bbmap/{wildcards.SAMPLE}/mapping_flag.txt"

else:
  
  rule mapping:
    input:
      fw = "{project}/samples/{SAMPLE}/rawdata/fw.fastq",
      rv = "{project}/samples/{SAMPLE}/rawdata/rv.fastq",
      contigs = "{project}/bbmap/{SAMPLE}/contigs_filtered.fasta",
      reference = "{project}/bbmap/{SAMPLE}/ref/genome/1/summary.txt"
    output:
      samfile = temp("{project}/bbmap/{SAMPLE}/{SAMPLE}.sam")
    shell:
      "bbmap.sh ref={input.contigs} "
      "path={wildcards.project}/bbmap/{wildcards.SAMPLE}"
      "in={input.fw} in2={input.rv} "
      "-out={output.samfile} "
      "-threads={mapping_threads}"
  
  rule bam_convert:
    input:
      samfile = "{project}/bbmap/{SAMPLE}/{SAMPLE}.sam"
    output:
      bamfile = temp("{project}/bbmap/{SAMPLE}/{SAMPLE}.bam")
    shell:
      "samtools view -b {input.samfile} "
      "-o {output.bamfile}"
  
  rule bam_sort:
    input:
      bamfile = "{project}/bbmap/{SAMPLE}/{SAMPLE}.bam"
    output:
      sortedbamfile = "{project}/bbmap/{SAMPLE}/{SAMPLE}_sorted.bam"
    shell:
      "samtools sort {input.bamfile} "
      "-o {output.sortedbamfile}"
  
  rule bam_index:
    input:
      sortedbamfile = "{project}/bbmap/{SAMPLE}/{SAMPLE}_sorted.bam"
    output:
      bamindexfile = "{project}/bbmap/{SAMPLE}/{SAMPLE}_sorted.bam.bai"
    shell:
      "samtools index -b -@ " + config["mapping"]["threads"] +
      " {input.sortedbamfile} "
      "{wildcards.project}/bbmap/{wildcards.SAMPLE}/{wildcards.SAMPLE}_sorted.bam.bai"
  
  #Same rule for the replicates to make sure both options can equally be used.
  rule request_mapping:
    input:
      rules.bam_index.output
    output:
      "{project}/bbmap/{SAMPLE}/mapping_flag.txt"
    shell:
      "touch {wildcards.project}/bbmap/{wildcards.SAMPLE}/mapping_flag.txt"

binning_kmerlen = config["binning"]["kmer_len"]
binning_threads = config["binning"]["threads"]
metabinner_env = config["binning"]["metabinner_env"]
comp_cutoff = config["binning"]["cutoff_comp"]
cont_cutoff = config["binning"]["cutoff_cont"]
#
###########
# Binning #
###########
# All the binning is done under here. Flags are used to make use all the mapping
# has been done correctly so that no samples get ignored.

#Generates the contig depth file from all alignments in the bbmap dir.
rule gen_depth:
  input:
    "{project}/bbmap/{SAMPLE}/mapping_flag.txt"
  output:
    depthfile = "{project}/metabinner/{SAMPLE}/mb2_master_depth.txt"
  shell:
    "./scripts/metabinner_scripts/jgi_summarize_bam_contig_depths "
    "--outputDepth {output.depthfile} "
    "--noIntraDepthVariance {wildcards.project}/bbmap/{wildcards.SAMPLE}/*.bam"

#Generates a coverage file from the contig depth file.
rule gen_coverage:
  input:
    depthfile = "{project}/metabinner/{SAMPLE}/mb2_master_depth.txt"
  output:
    coverage_profile = "{project}/metabinner/{SAMPLE}/coverage_profile.tsv"
  shell:
    """cat {input.depthfile} \
    | awk '{{if ($2>'"1000"') print $0 }}' | \
    cut -f -1,4- > {output.coverage_profile}"""


#Create kmer profile and run metabinner
rule metabinner:
  input:
    "{project}/bbmap/{SAMPLE}/contigs_filtered.fasta",
    "{project}/bbmap/{SAMPLE}/{SAMPLE}_sorted.bam.bai",
    "{project}/metabinner/{SAMPLE}/coverage_profile.tsv"
  output:
    "{project}/metabinner/{SAMPLE}/metabinner_res/result.log"
  shell:
    "bash ./scripts/mbinner.wrap "
    "{wildcards.project}/bbmap/{wildcards.SAMPLE}/contigs_filtered.fasta {min_contiglen} "
    "{binning_kmerlen} {wildcards.project}/metabinner/{wildcards.SAMPLE} "
    "{metabinner_env} {binning_threads}"


#This rule runs the checkM evaluation. For some reason, they decided to put the
#final bins in a very long directory chain, so while it looks weird, it's the correct
#bin directory.
rule checkm_eval:
  input:
    "{project}/metabinner/{SAMPLE}/metabinner_res/result.log"
  output:
    "{project}/checkm/{SAMPLE}/lineage.ms"
  shell:
    "bash ./scripts/checkm.wrap "
    "{binning_threads} "
    "{wildcards.project} "
    "{wildcards.SAMPLE}"


#Filter the bins based on the checkM evaluation. The cutoff can be changed in the config.
rule filter_bins:
  input:
    "{project}/checkm/{SAMPLE}/lineage.ms"
  output:
    "{project}/filtered_bins/{SAMPLE}/bin_stats.tsv"
  shell:
    "python3 scripts/filter_bins.py "
    "{wildcards.project}/checkm/{wildcards.SAMPLE}/storage/bin_stats_ext.tsv "
    "{comp_cutoff} {cont_cutoff} "
    "{wildcards.project}/metabinner/{wildcards.SAMPLE}/metabinner_res/ensemble_res/greedy_cont_weight_3_mincomp_50.0_maxcont_15.0_bins/ensemble_3logtrans/addrefined2and3comps/greedy_cont_weight_3_mincomp_50.0_maxcont_15.0_bins/ "
    "{wildcards.project}/filtered_bins/{wildcards.SAMPLE}/"

class_cpus = config["classification"]["cpus"]
#
################
#Classification
################
#

#Runs the classification with gtdbtk
rule gtdbtk:
  input:
    "{project}/filtered_bins/{SAMPLE}/bin_stats.tsv"
  output:
    "{project}/gtdbtk/{SAMPLE}/gtdbtk.log"
  shell:
    "bash ./scripts/gtdbtk.wrap {wildcards.project}/filtered_bins/{wildcards.SAMPLE}/ "
    "{wildcards.project}/gtdbtk/{wildcards.SAMPLE}/ {class_cpus}"


#Runs a script to collect the classifications into one file, which is more readable.
rule get_classification:
  input:
    "{project}/gtdbtk/{SAMPLE}/gtdbtk.log"
  output:
    "{project}/gtdbtk/{SAMPLE}/classification.tsv"
  shell:
    "python3 scripts/get_classifications.py "
    "{wildcards.project}/gtdbtk/{wildcards.SAMPLE}/classification.tsv "
    "{wildcards.project}/gtdbtk/{wildcards.SAMPLE}"



quant_threads = config["quantification"]["threads"]
###########
#Abundance estimation
###########
#Uses salmon on the contigs to calculate the TPM per contig.
#This is then matched back to the contigs in the next rule.
rule salm_quant:
  input:
    filt_contigs = "{project}/bbmap/{SAMPLE}/contigs_filtered.fasta",
    fw = "{project}/samples/{SAMPLE}/rawdata/fw.fastq",
    rv = "{project}/samples/{SAMPLE}/rawdata/rv.fastq"
  output:
    "{project}/quantification/{SAMPLE}/salmon/quant.sf"
  shell:
    "bash ./scripts/salmon.wrap "
    "{wildcards.SAMPLE} "
    "{input.filt_contigs} "
    "{input.fw} "
    "{input.rv} "
    "{wildcards.project}/quantification/{wildcards.SAMPLE}/salmon/ "
    "{quant_threads}"


#Runs the script to find all contigs not found in any of the final bins.
#This then "deflates" the quantification numbers in order to increase accuracy.
rule find_residuals:
  input:
    bin_filt_flag = "{project}/filtered_bins/{SAMPLE}/bin_stats.tsv",
    filt_contigs = "{project}/bbmap/{SAMPLE}/contigs_filtered.fasta",
    salm_out = "{project}/quantification/{SAMPLE}/salmon/quant.sf"
  output:
    unbinned_contigs = "{project}/quantification/{SAMPLE}/unbinned_contigs.txt"
  shell:
    "python3 ./scripts/find_residuals.py "
    "{wildcards.SAMPLE} "
    "{input.filt_contigs} "
    "{wildcards.project}/filtered_bins/{wildcards.SAMPLE}/ "
    "{wildcards.SAMPLE} "
    "{output.unbinned_contigs}"


#Get the read mapping info from the BAM files.
rule readmapping:
  input:
    bamfile = "{project}/bbmap/{SAMPLE}/{SAMPLE}_sorted.bam",
    salm_out = "{project}/quantification/{SAMPLE}/salmon/quant.sf"
  output:
    readmapping_file = "{project}/quantification/{SAMPLE}/readmapping.txt"
  shell:
    "bash ./scripts/get_read_mapping.sh "
    "{input.bamfile} "
    "{output.readmapping_file} "


#The abundance estimation
rule quantify:
  input:
    bin_filt_flag = "{project}/filtered_bins/{SAMPLE}/bin_stats.tsv",
    salmon_quant = "{project}/quantification/{SAMPLE}/salmon/quant.sf",
    unbinned_contigs = "{project}/quantification/{SAMPLE}/unbinned_contigs.txt",
    readmapping = "{project}/quantification/{SAMPLE}/readmapping.txt"
  output:
    quant_table = "{project}/quantification/{SAMPLE}/quant_table.txt"
  shell:
    "python3 ./scripts/quantify.py "
    "{wildcards.project}/filtered_bins/{wildcards.SAMPLE}/ "
    "{input.unbinned_contigs} "
    "{input.salmon_quant} "
    "{input.readmapping} "
    "{wildcards.SAMPLE} "
    "{output.quant_table}"
